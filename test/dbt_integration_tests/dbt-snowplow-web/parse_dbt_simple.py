#!/usr/bin/env python3
"""
Simple script to parse dbt results and load them into Snowflake database.
"""

import os
import sys
import re
import snowflake.connector
from datetime import datetime
from db_connections import create_snowflake_connection, create_embucket_connection

def parse_duration(duration_str):
    """Parse duration string and convert to seconds"""
    if not duration_str:
        return 0.0
    
    # Remove any non-numeric characters except decimal point and 'm' or 's'
    duration_str = duration_str.strip()
    
    # Handle minutes format (e.g., "1m 30s", "2m", "1.5m")
    if 'm' in duration_str:
        # Split by 'm' and 's' to get minutes and seconds parts
        parts = re.split(r'[ms]', duration_str)
        minutes = 0.0
        seconds = 0.0
        
        if len(parts) >= 2 and parts[0]:
            minutes = float(parts[0])
        if len(parts) >= 3 and parts[1]:
            seconds = float(parts[1])
        
        return minutes * 60 + seconds
    
    # Handle seconds format (e.g., "2.5s", "30s")
    elif 's' in duration_str:
        seconds_str = duration_str.replace('s', '')
        return float(seconds_str) if seconds_str else 0.0
    
    # If no unit specified, assume seconds
    else:
        return float(duration_str) if duration_str else 0.0

def parse_dbt_output(dbt_output, total_rows_generated=0, is_incremental_run=False, run_type='manual'):
    """Parse dbt output and extract model information."""
    results = []
    
    # Remove ANSI color codes
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    clean_output = ansi_escape.sub('', dbt_output)
    
    # Join wrapped lines
    lines = clean_output.split('\n')
    joined_lines = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        # If line ends with space and next line starts with [, join them
        if i + 1 < len(lines) and line.endswith(' ') and lines[i + 1].strip().startswith('['):
            line = line + lines[i + 1].strip()
            i += 1
        joined_lines.append(line)
        i += 1
    
    current_timestamp = datetime.now()
    
    # Extract metadata from dbt output
    dbt_version = "unknown"
    adapter_type = "unknown"
    total_models = 0
    pass_count = 0
    warn_count = 0
    error_count = 0
    skip_count = 0
    
    # Use the provided incremental run flag
    
    for line in joined_lines:
        # Extract dbt version
        if "Running with dbt=" in line:
            version_match = re.search(r'dbt=([\d.]+)', line)
            if version_match:
                dbt_version = version_match.group(1)
        
        # Extract adapter type
        if "adapter type:" in line:
            adapter_match = re.search(r'adapter type: (\w+)', line)
            if adapter_match:
                adapter_type = adapter_match.group(1)
        
        # Note: incremental run detection is now handled via command line parameter
        
        # Extract total counts
        if "Done. PASS=" in line:
            counts_match = re.search(r'PASS=(\d+) WARN=(\d+) ERROR=(\d+) SKIP=(\d+) TOTAL=(\d+)', line)
            if counts_match:
                pass_count = int(counts_match.group(1))
                warn_count = int(counts_match.group(2))
                error_count = int(counts_match.group(3))
                skip_count = int(counts_match.group(4))
                total_models = int(counts_match.group(5))
    
    # Parse all model results
    for line in joined_lines:
        line = line.strip()
        
        # Look for lines with model results
        if 'OK' in line and ('model' in line or 'seed' in line) and '[' in line and ']' in line:
            # Extract order number
            order_match = re.search(r'(\d+ of \d+)', line)
            order = order_match.group(1) if order_match else "unknown"
            
            # Extract model name using regex to find the pattern: model_type model_name
            model_name_match = re.search(r'(sql \w+ model|seed file) ([^\s]+)', line)
            if model_name_match:
                model_name = model_name_match.group(2)  # The model name part
            else:
                model_name = "unknown"
            
            # Extract duration - handle both seconds and minutes
            duration_match = re.search(r'(\d+\.?\d*[ms]?)\s*\]', line)
            duration_str = duration_match.group(1) if duration_match else "0s"
            duration = parse_duration(duration_str)
            
            # Extract rows affected
            rows_match = re.search(r'\[(SUCCESS|CREATE) (\d+)', line)
            rows = int(rows_match.group(2)) if rows_match else 0
            
            # Determine result type
            if 'SUCCESS' in line:
                result = 'SUCCESS'
            elif 'CREATE' in line:
                result = 'CREATE'
            else:
                result = 'OK'
            
            # Determine model type
            if 'seed file' in line:
                model_type = 'seed'
            elif 'incremental model' in line:
                model_type = 'incremental'
            elif 'table model' in line:
                model_type = 'table'
            else:
                model_type = 'model'
            
            results.append({
                'timestamp': current_timestamp,
                'model_name': model_name,
                'model_type': model_type,
                'result': result,
                'duration': duration,
                'rows_affected': rows,
                'order': order,
                'target': 'snowflake',
                'dbt_version': dbt_version,
                'adapter_type': adapter_type,
                'total_models': total_models,
                'pass_count': pass_count,
                'warn_count': warn_count,
                'error_count': error_count,
                'skip_count': skip_count,
                'number_of_rows_generated': total_rows_generated,
                'is_incremental_run': is_incremental_run,
                'run_type': run_type
            })
        
        # Look for ERROR lines
        elif 'ERROR' in line and 'model' in line and '[' in line and ']' in line:
            order_match = re.search(r'(\d+ of \d+)', line)
            order = order_match.group(1) if order_match else "unknown"
            
            # Extract model name using regex to find the pattern: model_type model_name
            model_name_match = re.search(r'(sql \w+ model|seed file) ([^\s]+)', line)
            if model_name_match:
                model_name = model_name_match.group(2)  # The model name part
            else:
                model_name = "unknown"
            
            # Extract duration - handle both seconds and minutes
            duration_match = re.search(r'(\d+\.?\d*[ms]?)\s*\]', line)
            duration_str = duration_match.group(1) if duration_match else "0s"
            duration = parse_duration(duration_str)
            
            results.append({
                'timestamp': current_timestamp,
                'model_name': model_name,
                'model_type': 'table',
                'result': 'ERROR',
                'duration': duration,
                'rows_affected': 0,
                'order': order,
                'target': 'snowflake',
                'dbt_version': dbt_version,
                'adapter_type': adapter_type,
                'total_models': total_models,
                'pass_count': pass_count,
                'warn_count': warn_count,
                'error_count': error_count,
                'skip_count': skip_count,
                'number_of_rows_generated': total_rows_generated,
                'is_incremental_run': is_incremental_run,
                'run_type': run_type
            })
        
        # Look for SKIP lines
        elif 'SKIP' in line and 'relation' in line:
            order_match = re.search(r'(\d+ of \d+)', line)
            order = order_match.group(1) if order_match else "unknown"
            
            # Extract model name from SKIP relation line
            relation_match = re.search(r'relation ([^\s]+)', line)
            model_name = relation_match.group(1) if relation_match else "unknown"
            
            results.append({
                'timestamp': current_timestamp,
                'model_name': model_name,
                'model_type': 'skipped',
                'result': 'SKIP',
                'duration': 0.0,
                'rows_affected': 0,
                'order': order,
                'target': 'snowflake',
                'dbt_version': dbt_version,
                'adapter_type': adapter_type,
                'total_models': total_models,
                'pass_count': pass_count,
                'warn_count': warn_count,
                'error_count': error_count,
                'skip_count': skip_count,
                'number_of_rows_generated': total_rows_generated,
                'is_incremental_run': is_incremental_run,
                'run_type': run_type
            })
    
    return results

def get_row_count_queries():
    """Get SQL queries to count rows for all dbt models from count_table_rows.sql file."""
    try:
        with open('count_table_rows.sql', 'r') as f:
            sql_content = f.read()
        
        # Split by UNION ALL to get individual queries
        queries = []
        lines = sql_content.split('\n')
        current_query = ""
        
        for line in lines:
            line = line.strip()
            if line.startswith('SELECT') and 'COUNT(*)' in line:
                # Start of a new query
                current_query = line
            elif line.startswith('FROM') and current_query:
                # Complete the query
                current_query += " " + line
                queries.append(current_query)
                current_query = ""
            elif current_query and not line.startswith('UNION ALL') and not line.startswith('--') and line:
                # Continue building the query
                current_query += " " + line
        
        return queries
    except FileNotFoundError:
        print("⚠ count_table_rows.sql file not found, using fallback queries")
        return [
            "SELECT 'PUBLIC_SNOWPLOW_MANIFEST_SNOWPLOW_MANIFEST.snowplow_web_dim_ga4_source_categories' AS table_name, COUNT(*) AS row_count FROM PUBLIC_SNOWPLOW_MANIFEST_SNOWPLOW_MANIFEST.snowplow_web_dim_ga4_source_categories",
            "SELECT 'PUBLIC_SNOWPLOW_MANIFEST_SNOWPLOW_MANIFEST.snowplow_web_dim_geo_country_mapping' AS table_name, COUNT(*) AS row_count FROM PUBLIC_SNOWPLOW_MANIFEST_SNOWPLOW_MANIFEST.snowplow_web_dim_geo_country_mapping",
            "SELECT 'PUBLIC_SNOWPLOW_MANIFEST_SNOWPLOW_MANIFEST.snowplow_web_dim_rfc_5646_language_mapping' AS table_name, COUNT(*) AS row_count FROM PUBLIC_SNOWPLOW_MANIFEST_SNOWPLOW_MANIFEST.snowplow_web_dim_rfc_5646_language_mapping"
        ]

def execute_row_count_queries(conn, queries):
    """Execute row count queries and return results. Handle missing tables gracefully."""
    cursor = conn.cursor()
    results = []
    successful_queries = 0
    failed_queries = 0
    
    for query in queries:
        try:
            cursor.execute(query)
            result = cursor.fetchone()
            if result:
                table_name, row_count = result
                results.append((table_name, row_count))
                print(f"✓ {table_name}: {row_count} rows")
                successful_queries += 1
            else:
                print(f"⚠ No result for query: {query}")
                failed_queries += 1
        except Exception as e:
            # Extract table name from query for better error reporting
            table_name = "unknown"
            if "FROM" in query:
                try:
                    # Extract table name from SELECT ... FROM table_name
                    parts = query.split("FROM")
                    if len(parts) > 1:
                        table_part = parts[1].strip().split()[0]
                        table_name = table_part
                except:
                    pass
            
            print(f"⚠ Table not found (skipping): {table_name} - {str(e).split('(')[0].strip()}")
            failed_queries += 1
            continue
    
    cursor.close()
    
    print(f"✓ Successfully counted {successful_queries} tables")
    if failed_queries > 0:
        print(f"⚠ Skipped {failed_queries} tables (not found or failed to create)")
    
    return results

def add_row_count_column_if_not_exists(conn, target):
    """Add row_count column to dbt_snowplow_results_models if it doesn't exist."""
    cursor = conn.cursor()
    
    try:
        # Check if column exists - use different parameter placeholders for different databases
        if target.lower() == 'snowflake':
            cursor.execute("""
                SELECT COUNT(*) 
                FROM INFORMATION_SCHEMA.COLUMNS 
                WHERE TABLE_NAME = 'DBT_SNOWPLOW_RESULTS_MODELS' 
                AND COLUMN_NAME = 'ROW_COUNT'
            """)
        else:  # embucket
            cursor.execute("""
                SELECT COUNT(*) 
                FROM INFORMATION_SCHEMA.COLUMNS 
                WHERE TABLE_NAME = 'DBT_SNOWPLOW_RESULTS_MODELS' 
                AND COLUMN_NAME = 'ROW_COUNT'
            """)
        
        column_exists = cursor.fetchone()[0] > 0
        
        if not column_exists:
            print("Adding row_count column to dbt_snowplow_results_models...")
            cursor.execute("ALTER TABLE dbt_snowplow_results_models ADD COLUMN row_count INTEGER")
            conn.commit()
            print("✓ row_count column added successfully")
        else:
            print("✓ row_count column already exists")
            
    except Exception as e:
        print(f"⚠ Error adding row_count column: {e}")
    
    cursor.close()

def update_row_counts_in_results_table(conn, results, target, run_id):
    """Update existing records in dbt_snowplow_results_models table with row counts."""
    cursor = conn.cursor()
    
    try:
        # Update each row count result
        for table_name, row_count in results:
            # Convert table name to match dbt model name format
            # e.g., "PUBLIC_SNOWPLOW_MANIFEST_DERIVED.snowplow_web_sessions" -> "public_snowplow_manifest_derived.snowplow_web_sessions"
            model_name = table_name.lower()
            
            # Both Snowflake and Embucket use Snowflake connector, so use %s placeholders
            update_query = """
            UPDATE dbt_snowplow_results_models 
            SET row_count = %s
            WHERE model_name = %s AND target = %s AND run_id = %s
            """
            
            cursor.execute(update_query, (
                row_count,
                model_name,
                target,
                run_id
            ))
            
            if cursor.rowcount > 0:
                print(f"✓ Updated {model_name}: {row_count} rows")
            else:
                print(f"⚠ No matching record found for {model_name}")
        
        print(f"✓ Updated row counts for {len(results)} models in dbt_snowplow_results_models")
        
    except Exception as e:
        print(f"⚠ Error updating row counts: {e}")
    
    cursor.close()

def create_results_table(conn, cursor):
    """Create the dbt_snowplow_results_models table if it doesn't exist."""
    create_table_sql = """
        CREATE TABLE IF NOT EXISTS dbt_snowplow_results_models (
        id INTEGER AUTOINCREMENT PRIMARY KEY,
        timestamp TIMESTAMP_NTZ,
        model_name STRING,
        model_type STRING,
        result STRING,
        duration_seconds FLOAT,
        rows_affected INTEGER,
        order_sequence STRING,
        target STRING,
        run_id STRING,
        dbt_version STRING,
        adapter_type STRING,
        total_models INTEGER,
        pass_count INTEGER,
        warn_count INTEGER,
        error_count INTEGER,
        skip_count INTEGER,
        number_of_rows_generated INTEGER,
        is_incremental_run BOOLEAN,
        row_count INTEGER,
        run_type STRING DEFAULT 'manual',
        downloaded_at TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
    )
    """
    
    cursor.execute(create_table_sql)
    conn.commit()
    print("✓ dbt_snowplow_results_models table created/verified")

def load_results_to_database(results, target='snowflake'):
    """Load parsed results into Snowflake database (always use Snowflake for storage)."""
    print(f"=== Loading dbt Results into SNOWFLAKE Database ===")
    print(f"Connecting to SNOWFLAKE...")
    
    try:
        # Always use Snowflake for storing results
        conn = create_snowflake_connection()
        cursor = conn.cursor()
        print(f"✓ Connected to SNOWFLAKE successfully")
        
        # Create table
        create_results_table(conn, cursor)
        
        # Generate run_id for this batch
        run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Set the same downloaded_at timestamp for all models in this run
        downloaded_at = datetime.now()
        
        # Insert results - both Snowflake and Embucket use Snowflake connector, so use %s placeholders
        insert_sql = """
        INSERT INTO dbt_snowplow_results_models 
        (timestamp, model_name, model_type, result, duration_seconds, rows_affected, order_sequence, target, run_id, dbt_version, adapter_type, total_models, pass_count, warn_count, error_count, skip_count, number_of_rows_generated, is_incremental_run, row_count, run_type, downloaded_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        for result in results:
            result['target'] = target
            try:
                cursor.execute(insert_sql, (
                    result['timestamp'],
                    result['model_name'],
                    result['model_type'],
                    result['result'],
                    result['duration'],
                    result['rows_affected'],
                    result['order'],
                    result['target'],
                    run_id,
                    result['dbt_version'],
                    result['adapter_type'],
                    result['total_models'],
                    result['pass_count'],
                    result['warn_count'],
                    result['error_count'],
                    result['skip_count'],
                    result['number_of_rows_generated'],
                    result['is_incremental_run'],
                    None,  # row_count will be updated later
                    result['run_type'],
                    downloaded_at
                ))
            except Exception as e:
                print(f"Error inserting result for {result['model_name']}: {e}")
                print(f"Target: {target}")
                print(f"Insert SQL: {insert_sql}")
                raise
        
        conn.commit()
        print(f"✓ Loaded {len(results)} dbt results into {target.upper()}")
        
        # Verify data - both Snowflake and Embucket use Snowflake connector, so use %s placeholders
        cursor.execute("SELECT COUNT(*) FROM dbt_snowplow_results_models WHERE run_id = %s", (run_id,))
        count = cursor.fetchone()[0]
        print(f"✓ Verification: {count} records loaded for run_id: {run_id}")
        
        # Show summary
        cursor.execute("""
            SELECT 
                result,
                COUNT(*) as count,
                AVG(duration_seconds) as avg_duration,
                SUM(rows_affected) as total_rows,
                MAX(number_of_rows_generated) as total_rows_generated,
                MAX(is_incremental_run) as is_incremental_run
            FROM dbt_snowplow_results_models 
            WHERE run_id = %s
            GROUP BY result
            ORDER BY result
        """, (run_id,))
        
        print("\n=== Results Summary ===")
        for row in cursor.fetchall():
            result, count, avg_duration, total_rows, total_rows_generated, is_incremental_run = row
            print(f"{result}: {count} models, avg duration: {avg_duration:.2f}s, total rows: {total_rows}, total rows generated: {total_rows_generated}, incremental run: {is_incremental_run}")
        
        # Add row_count column if it doesn't exist
        print("\n=== Adding Row Count Column ===")
        add_row_count_column_if_not_exists(conn, 'snowflake')  # Always use snowflake for storage
        
        # Get row count queries
        print("\n=== Counting Table Rows ===")
        queries = get_row_count_queries()
        print(f"Found {len(queries)} tables to count")
        
        # Execute row count queries on the target database
        print(f"Executing row count queries on {target.upper()}...")
        if target.lower() == 'snowflake':
            row_count_conn = conn  # Use the same Snowflake connection
        else:
            row_count_conn = create_embucket_connection()  # Create new Embucket connection for counting
        
        row_count_results = execute_row_count_queries(row_count_conn, queries)
        
        # Close the row counting connection if it's different
        if target.lower() != 'snowflake':
            row_count_conn.close()
        
        # Update existing records in dbt_snowplow_results_models table with row counts (always in Snowflake)
        print("\n=== Updating Row Counts ===")
        update_row_counts_in_results_table(conn, row_count_results, target, run_id)
        
        cursor.close()
        conn.close()
        print("\n=== Data Load Process Complete ===")
        
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)

def main():
    if len(sys.argv) < 4:
        print("Usage: python3 parse_dbt_simple.py <dbt_output_file> <number_of_rows_generated> <is_incremental_run> [target] [run_type]")
        print("Example: python3 parse_dbt_simple.py dbt_output.log 100 false snowflake manual")
        print("Example: python3 parse_dbt_simple.py dbt_output.log 100 false snowflake github_actions")
        sys.exit(1)
    
    dbt_output_file = sys.argv[1]
    total_rows_generated = int(sys.argv[2])
    is_incremental_run = sys.argv[3].lower() == 'true'
    target = sys.argv[4] if len(sys.argv) > 4 else 'snowflake'
    run_type = sys.argv[5] if len(sys.argv) > 5 else 'manual'
    
    # Read dbt output from file
    try:
        with open(dbt_output_file, 'r') as f:
            dbt_output = f.read()
    except FileNotFoundError:
        print(f"Error: File {dbt_output_file} not found")
        sys.exit(1)
    
    # Parse the output
    print("Parsing dbt output...")
    results = parse_dbt_output(dbt_output, total_rows_generated, is_incremental_run, run_type)
    print(f"✓ Parsed {len(results)} model results")
    
    # Load to database
    load_results_to_database(results, target)

if __name__ == "__main__":
    main()